{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9799020",
   "metadata": {},
   "source": [
    "Author: Lynn Menchaca\n",
    "\n",
    "Date: 23Nov2022\n",
    "\n",
    "Project: Mental Health Classifier from Tweets\n",
    "\n",
    "Resources:\n",
    "\n",
    "- Kaggle Data Set: Tweet Mental Health Classification\n",
    "    - https://www.kaggle.com/competitions/tweet-mental-health-classification/data?select=train.csv\n",
    "- Youtube: Ken Jee -> Data Science Project from Scratch - Part 4 (Exploratory Data Analysis)\n",
    "    - https://www.youtube.com/watch?v=QWgg4w1SpJ8&list=PL2zq7klxX5ASFejJj80ob9ZAnBHdz5O1t&index=4\n",
    "- plot confusion matrix\n",
    "    - https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55041e",
   "metadata": {},
   "source": [
    "The purpose of this project is design a model to identify mental health in tweets.\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "### Data Processing and Analysis\n",
    "- Initial Overview: size, collumns, data types, missing values\n",
    "- Make copy of train data to play with\n",
    "- Remove duplicate rows\n",
    "- Clean the tweets\n",
    "    - Remove any characters that are not letters\n",
    "    - Removing stop words: and, it, ... (don't have much importance)\n",
    "    - keep just stem words\n",
    "- Create a word cloud for each Mental Health Label\n",
    "\n",
    "### Feature Engineering\n",
    "- Combine Test and Train Data\n",
    "- Missing Data -> By end of this step all rows are filled in\n",
    "- Format Data: strings, temporal varialbes -> \n",
    "    By end of this step all columns are ready for final processing and cleaning\n",
    "- Treating Outliers\n",
    "- Scaling/Transformation\n",
    "- Encoding methods\n",
    "- Handel Imbalance Data set\n",
    "- Split Test and Train Data -> Export both to csv file for Feature Selection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d834266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbca0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sending solidarity whoever doctor manage incre...</td>\n",
       "      <td>Stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need see hair amp beard gat book appointment b...</td>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>next time meet someone new dont ask ask love</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprise someone love give la senza gift box r...</td>\n",
       "      <td>Lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raise hand junhoes ocean lotion life rent free...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels\n",
       "0  sending solidarity whoever doctor manage incre...  Stressed\n",
       "1  need see hair amp beard gat book appointment b...   Anxious\n",
       "2      next time meet someone new dont ask ask love     Normal\n",
       "3  surprise someone love give la senza gift box r...    Lonely\n",
       "4  raise hand junhoes ocean lotion life rent free...    Normal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Frames\n",
    "\n",
    "data_file_path = '/Users/lynnpowell/Documents/DS_Projects/Data_Files/Tweet_Mental_Health_Data/'\n",
    "df_test = pd.read_csv(data_file_path+'test.csv')\n",
    "df_train = pd.read_csv(data_file_path+'train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e612da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "(29992, 2)\n",
      "tweets    0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "test Data\n",
      "(7499, 2)\n",
      "id        0\n",
      "tweets    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Overview of data\n",
    "\n",
    "print('Train Data')\n",
    "print(df_train.shape)\n",
    "print(df_train.isnull().sum())\n",
    "print('\\n')\n",
    "print('test Data')\n",
    "print(df_test.shape)\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636bc0f",
   "metadata": {},
   "source": [
    "There is no missing data from both the test and training data. This makes sense since there is only one dependant and independant feature in the training data. If there was any missing data in the training data set the row would have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9b4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets    object\n",
      "labels    object\n",
      "dtype: object\n",
      "id         int64\n",
      "tweets    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49276d26",
   "metadata": {},
   "source": [
    "The test and training data do not need to be formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e91f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweets    labels\n",
      "97     human need job cant exist amp make art chill cat    Anxious\n",
      "116    sad thing disinformation truth come damage alr...  Stressed\n",
      "146    sad thing disinformation truth come damage alr...  Stressed\n",
      "164    hi ask kindness might big battle life dad need...   Anxious\n",
      "168    boy get slaughter helpless mother could scream...  Stressed\n",
      "...                                                  ...       ...\n",
      "29985  hey friend need another nominee weekstarcompet...   Anxious\n",
      "29986  sad newscloris insanely talented could make la...  Stressed\n",
      "29988  name muhammad asif farooqi im pakistan amp sta...    Lonely\n",
      "29989  moms mad wont go overnight skiing trip 5 peopl...  Stressed\n",
      "29991  jps gooooo amp suck yuh madda mi haaaaaaaaaaaa...    Lonely\n",
      "\n",
      "[10615 rows x 2 columns]\n",
      "number of duplicate rows:  10615\n"
     ]
    }
   ],
   "source": [
    "#creating a copy of the training data to investigate \n",
    "df_tweets = df_train.copy()\n",
    "# checking for duplicate data in the training data frame.\n",
    "print(df_tweets[df_tweets.duplicated()])\n",
    "print('number of duplicate rows: ', df_tweets[df_tweets.duplicated()].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9ff9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data before removing duplicate rows:  (29992, 2)\n",
      "Size of training data before removing duplicate rows:  (19377, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sending solidarity whoever doctor manage incre...</td>\n",
       "      <td>Stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>need see hair amp beard gat book appointment b...</td>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>next time meet someone new dont ask ask love</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>surprise someone love give la senza gift box r...</td>\n",
       "      <td>Lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>raise hand junhoes ocean lotion life rent free...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             tweets    labels\n",
       "0      0  sending solidarity whoever doctor manage incre...  Stressed\n",
       "1      1  need see hair amp beard gat book appointment b...   Anxious\n",
       "2      2      next time meet someone new dont ask ask love     Normal\n",
       "3      3  surprise someone love give la senza gift box r...    Lonely\n",
       "4      4  raise hand junhoes ocean lotion life rent free...    Normal"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the duplicate rows\n",
    "print('Size of training data before removing duplicate rows: ', df_tweets.shape)\n",
    "df_tweets.drop_duplicates(keep='first',inplace=True)\n",
    "print('Size of training data before removing duplicate rows: ', df_tweets.shape)\n",
    "\n",
    "#Reseting the index since rows were dropped\n",
    "df_tweets.reset_index(inplace=True)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d2a43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal      7340\n",
       "Anxious     5047\n",
       "Stressed    3556\n",
       "Lonely      3434\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168dd0f5",
   "metadata": {},
   "source": [
    "\n",
    "The training data does not appear to be unbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c50456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters from text in title column\n",
    "# Removing stop words: and, it, ... (don't have much importance)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "full_corpus_string = ''\n",
    "\n",
    "for i in range(0, len(df_tweets)):\n",
    "    #replacing any character that is not a/A through z/Z with a blank - used for split\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', df_tweets['tweets'][i])\n",
    "    #uniforms the list to make it easier to go threw each word\n",
    "    tweet = tweet.lower()\n",
    "    #split -> each word in to a list\n",
    "    tweet = tweet.split()\n",
    "    \n",
    "    #checking each character for stem and stopwords\n",
    "    #removing the stop words then performing the steming process\n",
    "    tweet = [ps.stem(word) for word in tweet if not word in stopwords.words('english')]\n",
    "    tweet = ' '.join(tweet)\n",
    "    corpus.append(tweet)\n",
    "    full_corpus_string += tweet + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c0d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send solidar whoever doctor manag incred distress situat calmli disinfo',\n",
       " 'need see hair amp beard gat book appoint barb meanwhil fit stroll enter pot joint life',\n",
       " 'next time meet someon new dont ask ask love',\n",
       " 'surpris someon love give la senza gift box rm join telegram wont miss updat surprisesomeoneyoulov lasenzamalaysia lasenza pslasenza pslasenzakl pslasenzamalaysia personalshopperlasenza',\n",
       " 'rais hand junho ocean lotion life rent free mind ikon domin stanworldikon']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The remaining words for each row.\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c2ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send solidar whoever doctor manag incred distress situat calmli disinfo need see hair amp beard gat book appoint barb meanwhil fit stroll enter pot joint life next time meet someon new dont ask ask love surpris someon love give la senza gift box rm join telegram wont miss updat surprisesomeoneyoulov lasenzamalaysia lasenza pslasenza pslasenzakl pslasenzamalaysia personalshopperlasenza rais hand junho ocean lotion life rent free mind ikon domin stanworldikon mariposa de barrio teach matter guy fo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All the words as one long string\n",
    "\n",
    "full_corpus_string[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d428adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweets    labels\n",
      "0  send solidar whoever doctor manag incred distr...  Stressed\n",
      "1  need see hair amp beard gat book appoint barb ...   Anxious\n",
      "2        next time meet someon new dont ask ask love    Normal\n",
      "3  surpris someon love give la senza gift box rm ...    Lonely\n",
      "4  rais hand junho ocean lotion life rent free mi...    Normal\n",
      "   index                                             tweets    labels\n",
      "0      0  sending solidarity whoever doctor manage incre...  Stressed\n",
      "1      1  need see hair amp beard gat book appointment b...   Anxious\n",
      "2      2      next time meet someone new dont ask ask love     Normal\n",
      "3      3  surprise someone love give la senza gift box r...    Lonely\n",
      "4      4  raise hand junhoes ocean lotion life rent free...    Normal\n"
     ]
    }
   ],
   "source": [
    "#create a data frame with cleaned tweets\n",
    "df_corpus_str = pd.DataFrame(corpus,\n",
    "                            columns = ['tweets'])\n",
    "df_corpus_str['labels'] = df_tweets['labels']\n",
    "\n",
    "print(df_corpus_str.head())\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c4dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>send solidar whoever doctor manag incred distr...</td>\n",
       "      <td>Stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need see hair amp beard gat book appoint barb ...</td>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>next time meet someon new dont ask ask love</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surpris someon love give la senza gift box rm ...</td>\n",
       "      <td>Lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rais hand junho ocean lotion life rent free mi...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets    labels\n",
       "0  send solidar whoever doctor manag incred distr...  Stressed\n",
       "1  need see hair amp beard gat book appoint barb ...   Anxious\n",
       "2        next time meet someon new dont ask ask love    Normal\n",
       "3  surpris someon love give la senza gift box rm ...    Lonely\n",
       "4  rais hand junho ocean lotion life rent free mi...    Normal"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d59c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a word cloud out of the single sting of cleaned words\n",
    "#stopwords=STOPWORDS\n",
    "\n",
    "#wc = WordCloud(background_color='white', random_state=3, stopwords=STOPWORDS, max_words=2500, width=800, height=1500)\n",
    "#wc.generate(full_corpus_string)\n",
    "#print(wc)\n",
    "\n",
    "#plt.figure(figsize=[10,10])\n",
    "#plt.imshow(wc, interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf6e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressed\n",
      "Anxious\n",
      "Normal\n",
      "Lonely\n"
     ]
    }
   ],
   "source": [
    "# Creating word clouds\n",
    "\n",
    "mental_health_labels = df_corpus_str.labels.unique().tolist()\n",
    "#mental_health_labels = ['Stressed']\n",
    "\n",
    "for label in mental_health_labels:\n",
    "    corpus_str_long = ''\n",
    "    print(label)\n",
    "    df = df_corpus_str[df_corpus_str.labels == label]\n",
    "    corpus_str_long = ' '.join(df['tweets'].tolist())\n",
    "    #print(corpus_str_long)\n",
    "    \n",
    "    #Word Cloud plot\n",
    "    #wc = WordCloud(background_color='white', random_state=3, stopwords=STOPWORDS, max_words=2500, width=800, height=1500)\n",
    "    #wc.generate(full_corpus_string)\n",
    "\n",
    "    #plt.figure(figsize=[10,10])\n",
    "    #plt.imshow(wc, interpolation='bilinear')\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc828be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressed\n",
      "[('sad', 1407), ('tire', 966), ('im', 938), ('get', 478), ('like', 308), ('peopl', 302), ('make', 276), ('go', 269), ('dont', 249), ('incred', 214), ('say', 212), ('feel', 209), ('see', 201), ('awak', 199), ('know', 196), ('one', 195), ('think', 190), ('time', 184), ('trauma', 180), ('hard', 174)]\n",
      "\n",
      "\n",
      "Anxious\n",
      "[('need', 3406), ('im', 2187), ('help', 1265), ('love', 877), ('get', 804), ('dont', 725), ('like', 564), ('go', 532), ('know', 523), ('alon', 475), ('okay', 433), ('want', 418), ('make', 399), ('peopl', 370), ('one', 368), ('time', 343), ('feel', 337), ('think', 329), ('say', 322), ('realli', 313)]\n",
      "\n",
      "\n",
      "Normal\n",
      "[('like', 401), ('get', 395), ('im', 367), ('one', 299), ('say', 293), ('go', 282), ('dont', 281), ('trump', 262), ('day', 253), ('love', 252), ('good', 240), ('know', 226), ('make', 224), ('time', 220), ('think', 212), ('peopl', 205), ('amp', 199), ('year', 198), ('new', 196), ('see', 192)]\n",
      "\n",
      "\n",
      "Lonely\n",
      "[('need', 2167), ('love', 794), ('im', 548), ('alon', 480), ('dont', 404), ('get', 386), ('like', 326), ('want', 281), ('go', 281), ('help', 278), ('know', 274), ('make', 259), ('one', 233), ('peopl', 224), ('time', 217), ('think', 199), ('feel', 192), ('id', 190), ('say', 190), ('realli', 166)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most common words\n",
    "\n",
    "mental_health_labels = df_corpus_str.labels.unique().tolist()\n",
    "#mental_health_labels = ['Stressed']\n",
    "\n",
    "for label in mental_health_labels:\n",
    "    corpus_str_long = ''\n",
    "    print(label)\n",
    "    df = df_corpus_str[df_corpus_str.labels == label]\n",
    "    corpus_str_long = ' '.join(df['tweets'].tolist())\n",
    "    #print(corpus_str_long)\n",
    "    \n",
    "    # split() returns list of all the words in the string\n",
    "    corpus_split = corpus_str_long.split()\n",
    "    #print(corpus_split)\n",
    "  \n",
    "    # Pass the split_it list to instance of Counter class.\n",
    "    corpus_count = Counter(corpus_split)\n",
    "  \n",
    "    # most_common() produces k frequently encountered\n",
    "    # input values and their respective counts.\n",
    "    most_occur = corpus_count.most_common(20)\n",
    "  \n",
    "    print(most_occur)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb3013",
   "metadata": {},
   "source": [
    "Just looking at the top 10 most common words for each mental health label. There is a lot of similar words with stressed, anxious and lonely the order is just a little different.\n",
    "\n",
    "Stressed: Sad tire I'm get like people make go don't incredible\n",
    "Anxious: Need I'm help love get don't like go know alone\n",
    "Lonely: Need love I'm alone don't get like want go help\n",
    "Normal: Like get i'm one say go don't Trump day love\n",
    "\n",
    "Stressed: Positive words - 2 (like, incredible)\n",
    "            Negative words - 3 (sad, tire, don't)\n",
    "Anxious: Positive words - 2 (love, like)\n",
    "            Negative words - 3 (need, help, don't, alone)\n",
    "Lonely: Positive words - 2 (love, like)\n",
    "            Negative words - 3 (need, alone, don't, help)\n",
    "Normal: Positive words - - 2 (love, like)\n",
    "            Negative words - 1 (don't)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cc4d4",
   "metadata": {},
   "source": [
    "### Machine Learning Models:\n",
    "- Bag of Words Model (Convert strings to a matrix)\n",
    "    - Count Vectorizer\n",
    "    - Tfidf Vectorizer\n",
    "    - Hashing Vectorizer\n",
    "- Dependant (X) and Independent (y) variables\n",
    "- Train Test Split\n",
    "- ML Models with Hyperparameter Tuning\n",
    "    - Multinomial NM Algorithm\n",
    "    - Passivie Aggressive Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a777ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19377, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying Countvectorizer\n",
    "# Creating the Bag of Words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features = 5000 -> all the words\n",
    "#ngram_range -> 1, 2 or 3 grouped words\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176cd142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19377,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the dependant variable\n",
    "y=df_tweets['labels']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841d4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d5ebaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': 5000,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 3),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the CountVectorizer\n",
    "\n",
    "#cv.get_feature_names()[:20]\n",
    "cv.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e7526f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.611\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t3/m14fghtd0fs5gl_s5tzd3h500000gn/T/ipykernel_1989/1890543988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy: %.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'REAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('accuracy: %.3f'%score)\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "plot_confusion_matrix(cm, classes=['FAKE','REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecbb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
